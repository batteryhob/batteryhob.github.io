---
layout: post
title: LLM ì—ì´ì „íŠ¸ì— ê°•í™”í•™ìŠµ ì ìš©í•˜ê¸° - ì´ë¡ ë¶€í„° ì‹¤ì „ê¹Œì§€
date: 2025-11-09 01:00:00 +0900
author: ì „ì§€í˜¸
tags: ai llm reinforcement-learning rl agent machine-learning rlhf research
excerpt: LLM ì—ì´ì „íŠ¸ê°€ ê³¼ê±° ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµí•˜ê³  ì ì§„ì ìœ¼ë¡œ ê°œì„ ë˜ë„ë¡ ë§Œë“œëŠ” ê°•í™”í•™ìŠµ ê¸°ë²•ë“¤ì„ ì•Œì•„ë´…ë‹ˆë‹¤. í•™ê³„ ì—°êµ¬ë¶€í„° ì‹¤ìš©ì  êµ¬í˜„ê¹Œì§€ ì™„ë²½ ê°€ì´ë“œ.
use_math: true
toc: true
description: LLM ì—ì´ì „íŠ¸ì— ê°•í™”í•™ìŠµì„ ì ìš©í•˜ëŠ” ë°©ë²• - RLHF, ReAct, Reflexion, ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ
lang: ko
ref: llm-agent-rl
---

## ë“¤ì–´ê°€ë©°

LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì´ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ìƒì„±ì„ ë„˜ì–´ **ì—ì´ì „íŠ¸**ë¡œì„œ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œëŒ€ê°€ ì™”ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë§Œìœ¼ë¡œëŠ” íŠ¹ì • ì‘ì—…ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚´ê¸° ì–´ë µìŠµë‹ˆë‹¤.

**ê°•í™”í•™ìŠµ(Reinforcement Learning, RL)**ì„ í™œìš©í•˜ë©´ LLM ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì ì§„ì ìœ¼ë¡œ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì´ë¡ ë¶€í„° ì‹¤ì „ êµ¬í˜„ê¹Œì§€ ëª¨ë“  ê²ƒì„ ë‹¤ë£¹ë‹ˆë‹¤.

## í•µì‹¬ ì•„ì´ë””ì–´: í•œ ì¤„ ìš”ì•½

> **"LLMì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ ê²°ê³¼ê°€ ì¢‹ì•˜ëŠ”ì§€ ë‚˜ë¹´ëŠ”ì§€ ê¸°ë¡í•´ë‘ê³ , ë‹¤ìŒë²ˆì—” ë” ì˜í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ"**

---

## 1. ì™œ LLM ì—ì´ì „íŠ¸ì— RLì´ í•„ìš”í•œê°€?

### LLMì˜ í•œê³„

**ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ë¬¸ì œì :**
- ì¼ë°˜ì ì¸ íŒ¨í„´ë§Œ í•™ìŠµ (íŠ¹ì • ì‘ì—…ì— ìµœì í™”ë˜ì§€ ì•ŠìŒ)
- ì‚¬ìš©ì í”¼ë“œë°±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì˜í•˜ê¸° ì–´ë ¤ì›€
- ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•œ ê°œì„ ì´ ë¶ˆê°€ëŠ¥
- íŒŒë¼ë¯¸í„°ê°€ ê³ ì •ë˜ì–´ ìˆì–´ ëŒ€í™” ì¤‘ í•™ìŠµ ë¶ˆê°€

### ê°•í™”í•™ìŠµì˜ í•´ê²°ì±…

**RLì´ ì œê³µí•˜ëŠ” ê²ƒ:**
- í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•œ í•™ìŠµ
- ë³´ìƒ ì‹ í˜¸ë¥¼ í†µí•œ í–‰ë™ ìµœì í™”
- ì‹œí–‰ì°©ì˜¤(trial-and-error)ë¥¼ í†µí•œ ì ì§„ì  ê°œì„ 
- ì¥ê¸°ì  ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì „ëµ í•™ìŠµ

---

## 2. LLMì€ ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ê°€?

ë¨¼ì € Claude, ChatGPT ê°™ì€ LLMì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì§€ëŠ”ì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

### RLHF (Reinforcement Learning from Human Feedback)

í˜„ëŒ€ LLMì˜ í‘œì¤€ í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤.

#### í•™ìŠµ íŒŒì´í”„ë¼ì¸

```
1. Pre-training (ì‚¬ì „ í•™ìŠµ)
   â†“
   ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì–¸ì–´ íŒ¨í„´ í•™ìŠµ

2. Supervised Fine-tuning (ì§€ë„ ë¯¸ì„¸ì¡°ì •)
   â†“
   ê³ í’ˆì§ˆ ëŒ€í™” ì˜ˆì‹œë¡œ ë¯¸ì„¸ì¡°ì •

3. Reward Model Training (ë³´ìƒ ëª¨ë¸ í•™ìŠµ)
   â†“
   ì¸ê°„ì´ ì—¬ëŸ¬ ì‘ë‹µì„ ë¹„êµ í‰ê°€
   "ì‘ë‹µ Aê°€ ì‘ë‹µ Bë³´ë‹¤ ì¢‹ë‹¤"
   â†’ ì¢‹ì€ ì‘ë‹µì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ í•™ìŠµ

4. RL Optimization (PPO ì•Œê³ ë¦¬ì¦˜)
   â†“
   - LLMì´ ì‘ë‹µ ìƒì„±
   - Reward Modelë¡œ ì ìˆ˜ ë§¤ê¹€
   - ì ìˆ˜ ë†’ì€ ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
```

#### í•µì‹¬ ë…¼ë¬¸

**InstructGPT (OpenAI, 2022)**
```python
# ê°œë…ì  ì½”ë“œ
def rlhf_training():
    # 1. ì‘ë‹µ ìƒì„±
    response = llm.generate(prompt)

    # 2. ë³´ìƒ ê³„ì‚°
    reward = reward_model.score(prompt, response)

    # 3. PPOë¡œ ì •ì±… ì—…ë°ì´íŠ¸
    loss = -reward * log_prob(response)
    llm.update(loss)
```

**Constitutional AI (Anthropic, 2022)**
- ì›ì¹™ ê¸°ë°˜ í•™ìŠµ
- Self-critiqueì™€ revision
- ì•ˆì „ì„±ê³¼ ìœ ìš©ì„± ê· í˜•

**í•µì‹¬ íŠ¹ì§•:**
- ì˜¤í”„ë¼ì¸ í•™ìŠµ (ë°°í¬ ì „ ì™„ë£Œ)
- ì‹¤ì œ ì‚¬ìš© ì‹œì—” íŒŒë¼ë¯¸í„° ê³ ì •
- ëŒ€í™” ì¤‘ í•™ìŠµí•˜ì§€ ì•ŠìŒ

---

## 3. LLM ì—ì´ì „íŠ¸ëŠ” ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€?

### ì—ì´ì „íŠ¸ì˜ íŠ¹ì§•

**ì¼ë°˜ LLM vs ì—ì´ì „íŠ¸:**

| íŠ¹ì§• | ì¼ë°˜ LLM | LLM ì—ì´ì „íŠ¸ |
|------|---------|-------------|
| ì‘ë™ ë°©ì‹ | ì¼íšŒì„± ì‘ë‹µ | ë°˜ë³µì  ìƒí˜¸ì‘ìš© |
| í™˜ê²½ | X | O (ë„êµ¬, API, íŒŒì¼ ì‹œìŠ¤í…œ) |
| í”¼ë“œë°± | í•™ìŠµ ì‹œì—ë§Œ | ì‹¤ì‹œê°„ |
| ë©”ëª¨ë¦¬ | ì»¨í…ìŠ¤íŠ¸ë§Œ | ì™¸ë¶€ ì €ì¥ì†Œ ê°€ëŠ¥ |
| ê°œì„  | ì¬í•™ìŠµ í•„ìš” | ëŸ°íƒ€ì„ ê°œì„  ê°€ëŠ¥ |

### ì—ì´ì „íŠ¸ê°€ ì§ë©´í•˜ëŠ” ë¬¸ì œ

```python
# ì—ì´ì „íŠ¸ì˜ ì‘ì—… ì˜ˆì‹œ
Task: "ì´ ë²„ê·¸ë¥¼ ê³ ì³ì¤˜"

Try 1:
  Action 1: íŒŒì¼ Aë§Œ ìˆ˜ì •
  Result: ì‹¤íŒ¨ (ë‹¤ë¥¸ íŒŒì¼ê³¼ ì—°ê´€ë¨)

Try 2:
  Action 1: ê´€ë ¨ íŒŒì¼ ê²€ìƒ‰
  Action 2: íŒŒì¼ A, B, C ëª¨ë‘ ìˆ˜ì •
  Result: ì„±ê³µ!
```

**ë¬¸ì œ:** ì¼ë°˜ LLMì€ Try 1ì˜ ì‹¤íŒ¨ë¥¼ ë‹¤ìŒë²ˆì— í™œìš©í•˜ì§€ ëª»í•©ë‹ˆë‹¤.

**í•´ê²°:** RL ê¸°ë²•ìœ¼ë¡œ ê²½í—˜ì„ í•™ìŠµí•˜ê³  ì¬ì‚¬ìš©!

---

## 4. ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ RL ì—°êµ¬ë“¤

### A. ReAct (2022)

**"Reasoning and Acting in Language Models"** - Yao et al.

#### í•µì‹¬ ì•„ì´ë””ì–´

ìƒê°(Thought)ê³¼ í–‰ë™(Action)ì„ ë²ˆê°ˆì•„ê°€ë©° ìˆ˜í–‰

```python
# ReAct íŒ¨í„´
loop:
  Thought: "ë²„ê·¸ë¥¼ ì°¾ìœ¼ë ¤ë©´ ì—ëŸ¬ ë¡œê·¸ë¥¼ ë¨¼ì € ë´ì•¼ê² ë‹¤"
  Action: read_file("error.log")
  Observation: "TypeError at line 45"

  Thought: "45ë²ˆ ì¤„ì„ í™•ì¸í•´ì•¼ê² ë‹¤"
  Action: read_file("main.py", line=45)
  Observation: "ë³€ìˆ˜ íƒ€ì…ì´ ì˜ëª»ë¨"

  Thought: "íƒ€ì…ì„ ìˆ˜ì •í•´ì•¼ê² ë‹¤"
  Action: edit_file("main.py", line=45, new_code="...")
  Observation: "ìˆ˜ì • ì™„ë£Œ"
```

#### RL ì—°ê²°ì 

- ê³¼ê±° ì„±ê³µí•œ trajectoryë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
- Few-shot learningìœ¼ë¡œ í–‰ë™ íŒ¨í„´ í•™ìŠµ
- ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ë°˜ë©´êµì‚¬ë¡œ í™œìš©

---

### B. Reflexion (2023)

**"Language Agents with Verbal Reinforcement Learning"** - Shinn et al.

#### í•µì‹¬ ì•„ì´ë””ì–´

ì‹¤íŒ¨ë¡œë¶€í„° í•™ìŠµí•˜ê³  ì–¸ì–´ë¡œ ëœ í”¼ë“œë°±ì„ ì €ì¥

```python
# Reflexion í”„ë¡œì„¸ìŠ¤
class ReflexionAgent:
    def __init__(self):
        self.memory = []  # ê³¼ê±° ê²½í—˜ ì €ì¥

    def solve_task(self, task):
        # 1. ê´€ë ¨ ê²½í—˜ ê²€ìƒ‰
        past_attempts = self.search_memory(task)

        # 2. ì‘ì—… ìˆ˜í–‰
        result = self.execute(task, context=past_attempts)

        # 3. Self-reflection (ì‹¤íŒ¨ ì‹œ)
        if not result.success:
            reflection = self.reflect(task, result)
            self.memory.append({
                'task': task,
                'attempt': result.actions,
                'outcome': 'failed',
                'reflection': reflection
            })

        return result

    def reflect(self, task, result):
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        prompt = f"""
        Task: {task}
        Actions taken: {result.actions}
        Error: {result.error}

        What went wrong and how to improve?
        """
        return llm.generate(prompt)
```

#### ì‹¤ì œ ì˜ˆì‹œ

```
Try 1:
  Task: "ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"
  Actions: [requests.get() ì§ì ‘ í˜¸ì¶œ]
  Result: 403 Forbidden
  Reflection: "User-Agent í—¤ë”ê°€ í•„ìš”í–ˆë‹¤.
               ë‹¤ìŒì—” í—¤ë”ë¥¼ ì„¤ì •í•´ì•¼ í•¨"

Try 2:
  Task: "ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"
  Previous reflection ì°¸ì¡°
  Actions: [User-Agent í¬í•¨í•œ requests.get()]
  Result: ì„±ê³µ!
```

#### RL ìš©ì–´ ë§¤í•‘

- **State**: í˜„ì¬ ì‘ì—… + ê³¼ê±° ê²½í—˜
- **Action**: LLMì´ ìƒì„±í•œ ì½”ë“œ/ëª…ë ¹
- **Reward**: ì„±ê³µ/ì‹¤íŒ¨
- **Policy**: Reflectionì„ ê³ ë ¤í•œ í–‰ë™ ì„ íƒ

---

### C. Voyager (2023)

**"An Open-Ended Embodied Agent with LLMs"** - Wang et al.

#### í•µì‹¬ ì•„ì´ë””ì–´

ì„±ê³µí•œ ì½”ë“œë¥¼ "ìŠ¤í‚¬"ë¡œ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©

```python
# Voyager ìŠ¤í‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
class SkillLibrary:
    def __init__(self):
        self.skills = {}

    def add_skill(self, name, code, context):
        """ì„±ê³µí•œ ì½”ë“œë¥¼ ìŠ¤í‚¬ë¡œ ì €ì¥"""
        self.skills[name] = {
            'code': code,
            'success_rate': 1.0,
            'context': context,
            'dependencies': []
        }

    def search_skills(self, task):
        """ì‘ì—…ì— ë§ëŠ” ìŠ¤í‚¬ ê²€ìƒ‰"""
        # ë²¡í„° ìœ ì‚¬ë„ë¡œ ê´€ë ¨ ìŠ¤í‚¬ ì°¾ê¸°
        return vector_search(task, self.skills)

    def compose_skills(self, task):
        """ì—¬ëŸ¬ ìŠ¤í‚¬ ì¡°í•©"""
        relevant_skills = self.search_skills(task)
        return combine(relevant_skills)

# ì‚¬ìš© ì˜ˆì‹œ
skill_library = SkillLibrary()

# ì²« ë²ˆì§¸ ì‘ì—…
task1 = "ë‚˜ë¬´ ìºê¸°"
code1 = generate_code(task1)
if execute(code1).success:
    skill_library.add_skill("mine_wood", code1, task1)

# ë‘ ë²ˆì§¸ ì‘ì—… (ìŠ¤í‚¬ ì¬ì‚¬ìš©)
task2 = "ì§‘ ì§“ê¸°"
wood_skill = skill_library.search_skills("ë‚˜ë¬´ í•„ìš”")
code2 = generate_code(task2, existing_skills=[wood_skill])
```

#### Minecraftì—ì„œì˜ ì‹¤ì œ í™œìš©

```python
# ì´ˆê¸°: ìŠ¤í‚¬ ì—†ìŒ
solve("ë‹¤ì´ì•„ëª¬ë“œ ì°¾ê¸°")
  â†’ ì‹¤íŒ¨ (ë„êµ¬ ì—†ìŒ)

# í•™ìŠµëœ ìŠ¤í‚¬ë“¤
skill_library = {
    "craft_pickaxe": {...},  # ê³¡ê´­ì´ ë§Œë“¤ê¸°
    "mine_stone": {...},      # ëŒ ìºê¸°
    "find_cave": {...},       # ë™êµ´ ì°¾ê¸°
}

# ì¬ì‹œë„: ìŠ¤í‚¬ ì¡°í•©
solve("ë‹¤ì´ì•„ëª¬ë“œ ì°¾ê¸°")
  1. craft_pickaxe()
  2. find_cave()
  3. mine_diamond()
  â†’ ì„±ê³µ!
```

#### RL ì—°ê²°ì 

- **Hierarchical RL**: ìŠ¤í‚¬ = ì˜µì…˜(Options)
- **Curriculum Learning**: ì‰¬ìš´ ìŠ¤í‚¬ë¶€í„° ë³µì¡í•œ ìŠ¤í‚¬ë¡œ
- **Transfer Learning**: í•œ ë²ˆ ë°°ìš´ ìŠ¤í‚¬ ì¬ì‚¬ìš©

---

### D. WebGPT (2021)

**ì˜¨ë¼ì¸ RLì˜ ì„ êµ¬ì** - OpenAI

#### í•µì‹¬ ì•„ì´ë””ì–´

ì›¹ ë¸Œë¼ìš°ì§•ì„ RL í™˜ê²½ìœ¼ë¡œ ë§Œë“¤ê¸°

```python
# WebGPT Environment
class WebEnvironment:
    actions = [
        "search(query)",      # ê²€ìƒ‰
        "click(link_id)",     # ë§í¬ í´ë¦­
        "scroll(direction)",  # ìŠ¤í¬ë¡¤
        "quote(text)",        # ì¸ìš©
        "answer(text)"        # ë‹µë³€ ì œì¶œ
    ]

    def step(self, action):
        # í–‰ë™ ì‹¤í–‰
        observation = execute_browser_action(action)

        # ë³´ìƒ (ì¸ê°„ í‰ê°€)
        reward = human_feedback.score(observation)

        return observation, reward

# RL í•™ìŠµ
for episode in range(num_episodes):
    state = env.reset(question)

    while not done:
        action = agent.choose_action(state)
        next_state, reward = env.step(action)

        # ì •ì±… ì—…ë°ì´íŠ¸
        agent.update(state, action, reward, next_state)
```

#### íŠ¹ì§•

- **ì‹¤ì‹œê°„ í•™ìŠµ**: ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ì¦‰ì‹œ ê°œì„ 
- **ë³´ìƒ í•¨ìˆ˜**: ë‹µë³€ì˜ ì •í™•ì„± + ì¸ìš© í’ˆì§ˆ
- **íƒìƒ‰**: ìƒˆë¡œìš´ ê²€ìƒ‰ ì „ëµ ì‹œë„

---

### E. In-Context Reinforcement Learning (2023)

**íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì—†ì´ í•™ìŠµí•˜ëŠ” ê²ƒì²˜ëŸ¼ í–‰ë™**

#### Algorithm Distillation

```python
# RL ì•Œê³ ë¦¬ì¦˜ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì‹œë®¬ë ˆì´ì…˜
def in_context_rl(task):
    prompt = """
    ë‹¹ì‹ ì€ ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

    ì´ì „ ì‹œë„ë“¤:
    Try 1: Action=A, Reward=-1 (ì‹¤íŒ¨)
    Try 2: Action=B, Reward=-1 (ì‹¤íŒ¨)
    Try 3: Action=C, Reward=+1 (ì„±ê³µ!)
    Try 4: Action=C, Reward=+1 (ì„±ê³µ!)

    íŒ¨í„´: Action Cê°€ ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.

    ì´ì œ ìƒˆë¡œìš´ ìƒí™©:
    {task}

    ì–´ë–¤ í–‰ë™ì„ ì„ íƒí•˜ê² ìŠµë‹ˆê¹Œ?
    """
    return llm.generate(prompt)
```

#### í•µì‹¬ í†µì°°

LLMì€ í”„ë¡¬í”„íŠ¸ì—ì„œ RL ì•Œê³ ë¦¬ì¦˜ì„ "í•™ìŠµ"í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- Exploration vs Exploitation
- Credit assignment
- Policy improvement

**ì¥ì :**
- íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”
- ë¹ ë¥¸ ì ì‘
- ë‹¤ì–‘í•œ ì‘ì—…ì— ë²”ìš©ì 

**ë‹¨ì :**
- ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
- ì¥ê¸° ë©”ëª¨ë¦¬ ë¶€ì¡±

---

## 5. ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ

ì´ì œ ì‹¤ì œë¡œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì„ ì•Œì•„ë´…ì‹œë‹¤.

### ë°©ë²• 1: ë‹¨ìˆœ ë©”ëª¨ë¦¬ ê¸°ë°˜ (ê°€ì¥ ì‰¬ì›€)

```python
import json
from typing import List, Dict
from datetime import datetime

class SimpleMemoryAgent:
    """ê³¼ê±° ê²½í—˜ì„ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸"""

    def __init__(self, memory_file='agent_memory.json'):
        self.memory_file = memory_file
        self.memory = self.load_memory()

    def load_memory(self) -> List[Dict]:
        """ë©”ëª¨ë¦¬ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.memory_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return []

    def save_memory(self):
        """ë©”ëª¨ë¦¬ íŒŒì¼ ì €ì¥"""
        with open(self.memory_file, 'w') as f:
            json.dump(self.memory, f, indent=2)

    def find_similar_experiences(self, task: str, top_k=3):
        """ìœ ì‚¬í•œ ê³¼ê±° ê²½í—˜ ê²€ìƒ‰"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì‹¤ì œë¡  ì„ë² ë”© ì‚¬ìš©)
        task_words = set(task.lower().split())

        scored = []
        for exp in self.memory:
            exp_words = set(exp['task'].lower().split())
            similarity = len(task_words & exp_words) / len(task_words | exp_words)
            scored.append((similarity, exp))

        scored.sort(reverse=True)
        return [exp for _, exp in scored[:top_k]]

    def execute_task(self, task: str):
        """ì‘ì—… ì‹¤í–‰"""
        # 1. ê³¼ê±° ê²½í—˜ ê²€ìƒ‰
        similar = self.find_similar_experiences(task)

        # 2. ì„±ê³µí•œ íŒ¨í„´ ìš°ì„  ì‚¬ìš©
        successful_patterns = [
            exp for exp in similar
            if exp['result'] == 'success'
        ]

        # 3. í–‰ë™ ì„ íƒ
        if successful_patterns:
            print(f"âœ… ê³¼ê±° ì„±ê³µ ê²½í—˜ ë°œê²¬: {len(successful_patterns)}ê°œ")
            action = self.use_successful_pattern(successful_patterns[0])
        else:
            print("ğŸ” ìƒˆë¡œìš´ ì‹œë„...")
            action = self.explore_new_approach(task)

        # 4. ì‹¤í–‰
        result = self.run_action(action)

        # 5. ë©”ëª¨ë¦¬ì— ì €ì¥
        self.memory.append({
            'task': task,
            'action': action,
            'result': 'success' if result else 'failure',
            'timestamp': datetime.now().isoformat(),
            'score': 1 if result else -1
        })

        self.save_memory()
        return result

    def use_successful_pattern(self, experience):
        """ì„±ê³µí•œ íŒ¨í„´ ì¬ì‚¬ìš©"""
        return experience['action']

    def explore_new_approach(self, task):
        """ìƒˆë¡œìš´ ì ‘ê·¼ íƒìƒ‰"""
        # LLMì—ê²Œ ë¬¼ì–´ë³´ê¸°
        return llm_generate(task)

    def run_action(self, action):
        """ì‹¤ì œ í–‰ë™ ì‹¤í–‰"""
        # êµ¬í˜„ í•„ìš”
        pass

# ì‚¬ìš© ì˜ˆì‹œ
agent = SimpleMemoryAgent()

# ì²« ë²ˆì§¸ ì‹œë„
agent.execute_task("Python íŒŒì¼ì˜ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •")
# â†’ ìƒˆë¡œìš´ ì‹œë„

# ë‘ ë²ˆì§¸ ì‹œë„ (ë¹„ìŠ·í•œ ì‘ì—…)
agent.execute_task("TypeScript íŒŒì¼ì˜ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •")
# â†’ ê³¼ê±° ì„±ê³µ ê²½í—˜ í™œìš©!
```

---

### ë°©ë²• 2: ë³´ìƒ ê¸°ë°˜ ìš°ì„ ìˆœìœ„

```python
from collections import defaultdict
import random

class RewardBasedAgent:
    """ë³´ìƒ ì ìˆ˜ë¡œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ì—ì´ì „íŠ¸"""

    def __init__(self, epsilon=0.2):
        self.action_scores = defaultdict(lambda: {'total': 0, 'count': 0})
        self.epsilon = epsilon  # íƒí—˜ ë¹„ìœ¨

    def get_action_value(self, action):
        """í–‰ë™ì˜ í‰ê·  ë³´ìƒ ê³„ì‚°"""
        stats = self.action_scores[action]
        if stats['count'] == 0:
            return 0  # ì‹œë„í•œ ì  ì—†ìœ¼ë©´ 0
        return stats['total'] / stats['count']

    def choose_action(self, available_actions):
        """Epsilon-greedy ì „ëµìœ¼ë¡œ í–‰ë™ ì„ íƒ"""

        # íƒí—˜ (Exploration)
        if random.random() < self.epsilon:
            action = random.choice(available_actions)
            print(f"ğŸ” íƒí—˜: {action}")
            return action

        # í™œìš© (Exploitation)
        best_action = max(
            available_actions,
            key=self.get_action_value
        )
        print(f"ğŸ¯ í™œìš©: {best_action} (ì ìˆ˜: {self.get_action_value(best_action):.2f})")
        return best_action

    def update_score(self, action, reward):
        """í–‰ë™ì˜ ë³´ìƒ ì—…ë°ì´íŠ¸"""
        self.action_scores[action]['total'] += reward
        self.action_scores[action]['count'] += 1

        avg = self.get_action_value(action)
        print(f"ğŸ“Š {action} ì—…ë°ì´íŠ¸: í‰ê·  ë³´ìƒ = {avg:.2f}")

# ì‚¬ìš© ì˜ˆì‹œ
agent = RewardBasedAgent(epsilon=0.2)

# ë²„ê·¸ ìˆ˜ì • ì‹œë‚˜ë¦¬ì˜¤
actions = [
    "ë‹¨ì¼ íŒŒì¼ë§Œ ìˆ˜ì •",
    "ê´€ë ¨ íŒŒì¼ ëª¨ë‘ ê²€ìƒ‰ í›„ ìˆ˜ì •",
    "í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„± í›„ ìˆ˜ì •"
]

for episode in range(10):
    action = agent.choose_action(actions)

    # ì‹¤í–‰ ë° ë³´ìƒ
    if action == "í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„± í›„ ìˆ˜ì •":
        reward = 1  # ë†’ì€ ì„±ê³µë¥ 
    elif action == "ê´€ë ¨ íŒŒì¼ ëª¨ë‘ ê²€ìƒ‰ í›„ ìˆ˜ì •":
        reward = 0.5  # ì¤‘ê°„
    else:
        reward = -0.5  # ë‚®ì€ ì„±ê³µë¥ 

    agent.update_score(action, reward)

# ê²°ê³¼: "í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„±"ì´ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì–»ìŒ
```

---

### ë°©ë²• 3: Q-Learning ê¸°ë°˜ (ê³ ê¸‰)

```python
class QLearningAgent:
    """Q-learningìœ¼ë¡œ ìµœì  ì •ì±… í•™ìŠµ"""

    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.2):
        self.q_table = {}  # (state, action) -> Q-value
        self.alpha = alpha  # í•™ìŠµë¥ 
        self.gamma = gamma  # í• ì¸ìœ¨
        self.epsilon = epsilon  # íƒí—˜ ë¹„ìœ¨

    def get_q_value(self, state, action):
        """Q-value ì¡°íšŒ"""
        return self.q_table.get((state, action), 0.0)

    def choose_action(self, state, available_actions):
        """Epsilon-greedyë¡œ í–‰ë™ ì„ íƒ"""

        # íƒí—˜
        if random.random() < self.epsilon:
            return random.choice(available_actions)

        # í™œìš©: ìµœëŒ€ Q-value í–‰ë™ ì„ íƒ
        q_values = [
            (action, self.get_q_value(state, action))
            for action in available_actions
        ]
        return max(q_values, key=lambda x: x[1])[0]

    def update(self, state, action, reward, next_state, next_actions):
        """Q-learning ì—…ë°ì´íŠ¸"""

        # í˜„ì¬ Q-value
        old_q = self.get_q_value(state, action)

        # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Q-value
        if next_actions:
            max_next_q = max(
                self.get_q_value(next_state, a)
                for a in next_actions
            )
        else:
            max_next_q = 0  # ì¢…ë£Œ ìƒíƒœ

        # Q-learning ì—…ë°ì´íŠ¸ ê³µì‹
        new_q = old_q + self.alpha * (reward + self.gamma * max_next_q - old_q)

        self.q_table[(state, action)] = new_q

        print(f"Q({state}, {action}): {old_q:.2f} â†’ {new_q:.2f}")

# ì‹¤ì „ ì˜ˆì‹œ: ì½”ë“œ ë””ë²„ê¹… ì—ì´ì „íŠ¸
agent = QLearningAgent()

# ìƒíƒœ: ì—ëŸ¬ íƒ€ì…
# í–‰ë™: ë””ë²„ê¹… ì „ëµ
states = ["TypeError", "ValueError", "AttributeError"]
actions = {
    "TypeError": ["íƒ€ì… ì²´í¬ ì¶”ê°€", "íƒ€ì… ë³€í™˜", "ì¸í„°í˜ì´ìŠ¤ ìˆ˜ì •"],
    "ValueError": ["ì…ë ¥ ê²€ì¦", "ì˜ˆì™¸ ì²˜ë¦¬", "ê¸°ë³¸ê°’ ì„¤ì •"],
    "AttributeError": ["ì†ì„± ì¡´ì¬ í™•ì¸", "ì´ˆê¸°í™” ìˆ˜ì •", "Null ì²´í¬"]
}

# í•™ìŠµ
for episode in range(100):
    state = random.choice(states)
    action = agent.choose_action(state, actions[state])

    # ì‹œë®¬ë ˆì´ì…˜: íŠ¹ì • í–‰ë™ì´ ë” íš¨ê³¼ì 
    if (state == "TypeError" and action == "ì¸í„°í˜ì´ìŠ¤ ìˆ˜ì •") or \
       (state == "ValueError" and action == "ì…ë ¥ ê²€ì¦") or \
       (state == "AttributeError" and action == "Null ì²´í¬"):
        reward = 1
        next_state = None  # ì„±ê³µ
    else:
        reward = -0.1
        next_state = state  # ê³„ì†

    next_actions = actions[next_state] if next_state else []
    agent.update(state, action, reward, next_state, next_actions)

# í•™ìŠµëœ ì •ì±… í…ŒìŠ¤íŠ¸
print("\nìµœì  ì •ì±…:")
for state in states:
    best_action = agent.choose_action(state, actions[state])
    print(f"{state} â†’ {best_action}")
```

---

### ë°©ë²• 4: ë²¡í„° DB + ì„ë² ë”© (ì‹¤ìš©ì )

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class VectorMemoryAgent:
    """ì„ë² ë”© ê¸°ë°˜ ê²½í—˜ ê²€ìƒ‰"""

    def __init__(self):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.experiences = []
        self.embeddings = []

    def add_experience(self, task, action, result, reflection=""):
        """ê²½í—˜ ì¶”ê°€"""
        experience = {
            'task': task,
            'action': action,
            'result': result,
            'reflection': reflection
        }

        # ì„ë² ë”© ìƒì„±
        text = f"{task} {action} {reflection}"
        embedding = self.encoder.encode(text)

        self.experiences.append(experience)
        self.embeddings.append(embedding)

    def find_similar(self, query, top_k=3, min_similarity=0.3):
        """ìœ ì‚¬í•œ ê²½í—˜ ê²€ìƒ‰"""
        if not self.experiences:
            return []

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.encoder.encode(query)

        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(
            [query_embedding],
            self.embeddings
        )[0]

        # Top-K ì„ íƒ
        indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in indices:
            if similarities[idx] >= min_similarity:
                results.append({
                    **self.experiences[idx],
                    'similarity': similarities[idx]
                })

        return results

    def solve_with_memory(self, task):
        """ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ë¬¸ì œ í•´ê²°"""
        # 1. ìœ ì‚¬í•œ ê³¼ê±° ê²½í—˜ ê²€ìƒ‰
        similar_exps = self.find_similar(task, top_k=3)

        if similar_exps:
            print(f"ğŸ“š {len(similar_exps)}ê°œì˜ ê´€ë ¨ ê²½í—˜ ë°œê²¬:")
            for i, exp in enumerate(similar_exps, 1):
                print(f"  {i}. {exp['task'][:50]}... "
                      f"(ìœ ì‚¬ë„: {exp['similarity']:.2f}, "
                      f"ê²°ê³¼: {exp['result']})")

        # 2. ì„±ê³µ ê²½í—˜ ìš°ì„  í™œìš©
        successful = [e for e in similar_exps if e['result'] == 'success']

        if successful:
            print(f"âœ… ì„±ê³µ ê²½í—˜ í™œìš©!")
            base_action = successful[0]['action']
        else:
            print(f"ğŸ†• ìƒˆë¡œìš´ ì ‘ê·¼ í•„ìš”")
            base_action = self.generate_new_action(task)

        return base_action

    def generate_new_action(self, task):
        """ìƒˆë¡œìš´ í–‰ë™ ìƒì„± (LLM í˜¸ì¶œ)"""
        # ì‹¤ì œë¡œëŠ” LLM ì‚¬ìš©
        return f"ìƒˆ ì „ëµ: {task}"

# ì‚¬ìš© ì˜ˆì‹œ
agent = VectorMemoryAgent()

# ê²½í—˜ ì¶•ì 
agent.add_experience(
    task="React ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ ì—ëŸ¬ ìˆ˜ì •",
    action="useEffect ì˜ì¡´ì„± ë°°ì—´ ìˆ˜ì •",
    result="success",
    reflection="ì˜ì¡´ì„± ë°°ì—´ì´ ë¹„ì–´ìˆì–´ì„œ ë¬´í•œ ë£¨í”„ ë°œìƒí–ˆìŒ"
)

agent.add_experience(
    task="Vue ì»´í¬ë„ŒíŠ¸ ì—…ë°ì´íŠ¸ ì•ˆ ë¨",
    action="reactive() ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½",
    result="success",
    reflection="ê°ì²´ ì†ì„± ë³€ê²½ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŒ"
)

agent.add_experience(
    task="React ìƒíƒœ ì—…ë°ì´íŠ¸ ì—ëŸ¬",
    action="setState í•¨ìˆ˜í˜• ì—…ë°ì´íŠ¸ ì‚¬ìš©",
    result="success",
    reflection="ì´ì „ ìƒíƒœ ê¸°ë°˜ ì—…ë°ì´íŠ¸ í•„ìš”"
)

# ìƒˆ ë¬¸ì œ í•´ê²°
new_task = "React Hook ì˜ì¡´ì„± ê²½ê³  í•´ê²°"
solution = agent.solve_with_memory(new_task)
# â†’ "React ì»´í¬ë„ŒíŠ¸ ë Œë”ë§ ì—ëŸ¬" ê²½í—˜ì´ ìœ ì‚¬í•˜ë‹¤ê³  ê²€ìƒ‰ë¨!
```

---

## 6. ì‹¤ìƒí™œ ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°

### ìš”ë¦¬ì‚¬ ì—ì´ì „íŠ¸

```
ì—ì´ì „íŠ¸ = ìš”ë¦¬ì‚¬
ì‘ì—… = íŒŒìŠ¤íƒ€ ë§Œë“¤ê¸°
í–‰ë™ = ì¬ë£Œ ì„ íƒ, ì¡°ë¦¬ ë°©ë²•
ë³´ìƒ = ë§› í‰ê°€

Episode 1:
  Action: ì†Œê¸ˆ 1ìŠ¤í‘¼
  Feedback: "ë„ˆë¬´ ì‹±ê±°ì›Œ" (-1ì )
  Memory: "ì†Œê¸ˆ 1ìŠ¤í‘¼ = ì‹¤íŒ¨"

Episode 2:
  Memory ì°¸ì¡°: "ì§€ë‚œë²ˆ 1ìŠ¤í‘¼ì€ ì•ˆëì–´"
  Action: ì†Œê¸ˆ 3ìŠ¤í‘¼
  Feedback: "ì™„ë²½í•´!" (+1ì )
  Memory: "ì†Œê¸ˆ 3ìŠ¤í‘¼ = ì„±ê³µ"

Episode 3 ì´í›„:
  Memory ì°¸ì¡°: "ì†Œê¸ˆ 3ìŠ¤í‘¼ì´ ìµœì "
  Action: ìë™ìœ¼ë¡œ 3ìŠ¤í‘¼ ì‚¬ìš©
  Feedback: ê³„ì† ì¢‹ì€ í‰ê°€

â†’ ì´ê²Œ ê°•í™”í•™ìŠµ!
```

---

## 7. ì£¼ìš” ì—°êµ¬ ë¹„êµí‘œ

| ë°©ë²•ë¡  | í•™ìŠµ ë°©ì‹ | ë©”ëª¨ë¦¬ | ì‹¤ì‹œê°„ í•™ìŠµ | ì¥ì  | ë‹¨ì  |
|--------|----------|--------|------------|------|------|
| **RLHF** | ì˜¤í”„ë¼ì¸ RL | X | X | ì•ˆì „, ê³ í’ˆì§ˆ | ë°°í¬ í›„ ê³ ì • |
| **ReAct** | Prompting | ì»¨í…ìŠ¤íŠ¸ ë‚´ | X | ê°„ë‹¨, í•´ì„ ê°€ëŠ¥ | ì§§ì€ ë©”ëª¨ë¦¬ |
| **Reflexion** | Episodic | í…ìŠ¤íŠ¸ ì €ì¥ | â–³ | ì‹¤íŒ¨ í•™ìŠµ | ì €ì¥ ê³µê°„ |
| **Voyager** | ìŠ¤í‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì½”ë“œ ì €ì¥ | â–³ | ì¬ì‚¬ìš©ì„± | ë„ë©”ì¸ íŠ¹í™” |
| **In-Context RL** | Few-shot | ì»¨í…ìŠ¤íŠ¸ | X | ë¹ ë¥¸ ì ì‘ | ê¸¸ì´ ì œí•œ |
| **WebGPT** | ì˜¨ë¼ì¸ RL | X | O | ì§€ì† ê°œì„  | ë¹„ìš©, ì•ˆì „ì„± |

---

## 8. êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­

### A. ë³´ìƒ ì„¤ê³„ (Reward Shaping)

```python
# ë‚˜ìœ ë³´ìƒ
reward = 1 if task_complete else 0  # Sparse reward

# ì¢‹ì€ ë³´ìƒ
reward = 0
if task_complete:
    reward += 1.0
if test_passed:
    reward += 0.5
if code_quality_good:
    reward += 0.3
if fast_execution:
    reward += 0.2
# Dense reward!
```

### B. íƒí—˜ vs í™œìš©

```python
def epsilon_greedy(epsilon, iteration):
    """ì ì§„ì ìœ¼ë¡œ íƒí—˜ ê°ì†Œ"""
    return epsilon * (0.99 ** iteration)

# ì´ˆê¸°: epsilon=0.5 (50% íƒí—˜)
# 100íšŒ í›„: epsilon=0.18 (18% íƒí—˜)
# 500íšŒ í›„: epsilon=0.003 (ê±°ì˜ í™œìš©ë§Œ)
```

### C. ì•ˆì „ì¥ì¹˜

```python
class SafeAgent:
    def execute_action(self, action):
        # 1. ìœ„í—˜ í–‰ë™ í•„í„°ë§
        if is_dangerous(action):
            print("âš ï¸ ìœ„í—˜í•œ í–‰ë™ ì°¨ë‹¨")
            return None

        # 2. ìƒŒë“œë°•ìŠ¤ì—ì„œ í…ŒìŠ¤íŠ¸
        test_result = run_in_sandbox(action)
        if not test_result.safe:
            print("âš ï¸ ì•ˆì „í•˜ì§€ ì•Šì€ ê²°ê³¼")
            return None

        # 3. ì‹¤ì œ ì‹¤í–‰
        return execute(action)
```

---

## 9. ë¯¸ë˜ ë°©í–¥

### A. íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬

```python
# ê³„ì¸µì  ë©”ëª¨ë¦¬
class HierarchicalMemory:
    def __init__(self):
        self.short_term = []  # ìµœê·¼ 10ê°œ
        self.long_term_index = VectorDB()  # ì¤‘ìš”í•œ ê²ƒë§Œ
        self.skill_library = {}  # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í‚¬

    def add(self, experience):
        self.short_term.append(experience)

        # ì¤‘ìš”ë„ í‰ê°€
        if is_important(experience):
            self.long_term_index.add(experience)

        # ìŠ¤í‚¬ ì¶”ì¶œ
        if is_reusable(experience):
            skill = extract_skill(experience)
            self.skill_library[skill.name] = skill
```

### B. ë©€í‹°ëª¨ë‹¬ í™•ì¥

```python
# ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ê²½í—˜
experience = {
    'task': "UI ë²„ê·¸ ìˆ˜ì •",
    'screenshot_before': img_before,
    'screenshot_after': img_after,
    'code_changes': diff,
    'result': 'success'
}
```

### C. í˜‘ì—… í•™ìŠµ

```python
# ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ ê²½í—˜ ê³µìœ 
class SharedMemory:
    def __init__(self):
        self.global_experiences = []

    def contribute(self, agent_id, experience):
        """ì—ì´ì „íŠ¸ê°€ ê²½í—˜ ê³µìœ """
        self.global_experiences.append({
            'agent': agent_id,
            'experience': experience
        })

    def learn_from_others(self, agent_id):
        """ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ê²½í—˜ í•™ìŠµ"""
        others = [
            exp for exp in self.global_experiences
            if exp['agent'] != agent_id
        ]
        return others
```

---

## 10. ì‹¤ì „ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´

### ì´ˆê¸‰: ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ë´‡

```python
# GitHub ì´ìŠˆ í•´ê²° ë´‡
class IssueResolverBot:
    def solve_issue(self, issue):
        # 1. ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰
        similar = self.find_similar_issues(issue)

        # 2. í•´ê²°ì±… ì ìš©
        if similar and similar[0].solved:
            solution = similar[0].solution
        else:
            solution = llm_generate_solution(issue)

        # 3. ê²°ê³¼ ì €ì¥
        self.save_experience(issue, solution)
```

### ì¤‘ê¸‰: ì½”ë“œ ë¦¬ë·° ì—ì´ì „íŠ¸

```python
class CodeReviewAgent:
    def review(self, pull_request):
        # 1. ê³¼ê±° ë¦¬ë·° íŒ¨í„´ í•™ìŠµ
        patterns = self.learn_from_past_reviews()

        # 2. ì ìš©
        issues = self.detect_issues(pull_request, patterns)

        # 3. í”¼ë“œë°±ìœ¼ë¡œ ê°œì„ 
        if developer_accepted:
            self.reinforce_pattern(issues, +1)
        else:
            self.reinforce_pattern(issues, -1)
```

### ê³ ê¸‰: ìë™ ë²„ê·¸ ìˆ˜ì • ì‹œìŠ¤í…œ

```python
class AutoBugFixer:
    def __init__(self):
        self.q_agent = QLearningAgent()
        self.memory = VectorMemoryAgent()

    def fix_bug(self, error_log):
        # 1. ì—ëŸ¬ ë¶„ë¥˜
        error_type = classify_error(error_log)

        # 2. ì „ëµ ì„ íƒ (Q-learning)
        strategies = self.get_strategies(error_type)
        strategy = self.q_agent.choose_action(error_type, strategies)

        # 3. ê³¼ê±° ê²½í—˜ ì°¸ì¡°
        similar_fixes = self.memory.find_similar(error_log)

        # 4. ìˆ˜ì • ì‹œë„
        fix = self.generate_fix(strategy, similar_fixes)
        result = self.test_fix(fix)

        # 5. í•™ìŠµ
        reward = 1 if result.passed else -1
        self.q_agent.update(error_type, strategy, reward, ...)
        self.memory.add_experience(error_log, fix, result)
```

---

## ë§ˆì¹˜ë©°

LLM ì—ì´ì „íŠ¸ì— ê°•í™”í•™ìŠµì„ ì ìš©í•˜ëŠ” ê²ƒì€ ë‹¨ìˆœíˆ í•™ìˆ ì  ê´€ì‹¬ì‚¬ê°€ ì•„ë‹ˆë¼ **ì‹¤ìš©ì ì¸ í•„ìš”**ì…ë‹ˆë‹¤.

### í•µì‹¬ ì •ë¦¬

1. **RLHF**: LLM ìì²´ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” í‘œì¤€ ë°©ë²•
2. **ReAct**: ìƒê°ê³¼ í–‰ë™ì„ ê²°í•©í•˜ì—¬ ë” ë‚˜ì€ ì¶”ë¡ 
3. **Reflexion**: ì‹¤íŒ¨ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ ì ì§„ì  ê°œì„ 
4. **Voyager**: ìŠ¤í‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì¬ì‚¬ìš©ì„± ê·¹ëŒ€í™”
5. **In-Context RL**: íŒŒë¼ë¯¸í„° ë³€ê²½ ì—†ì´ ì ì‘

### ì‹œì‘í•˜ê¸° ì¢‹ì€ ë°©ë²•

```
1. ë‹¨ìˆœ ë©”ëª¨ë¦¬ë¶€í„° ì‹œì‘
   â†’ JSON íŒŒì¼ì— ê²½í—˜ ì €ì¥

2. ìœ ì‚¬ë„ ê²€ìƒ‰ ì¶”ê°€
   â†’ ì„ë² ë”©ìœ¼ë¡œ ê´€ë ¨ ê²½í—˜ ì°¾ê¸°

3. ë³´ìƒ ì‹œìŠ¤í…œ ë„ì…
   â†’ ì„±ê³µ/ì‹¤íŒ¨ ì ìˆ˜ ë§¤ê¸°ê¸°

4. Q-learningìœ¼ë¡œ í™•ì¥
   â†’ ìµœì  ì „ëµ ìë™ í•™ìŠµ
```

### ì°¸ê³  ìë£Œ

- [ReAct ë…¼ë¬¸](https://arxiv.org/abs/2210.03629)
- [Reflexion ë…¼ë¬¸](https://arxiv.org/abs/2303.11366)
- [Voyager ë…¼ë¬¸](https://arxiv.org/abs/2305.16291)
- [InstructGPT ë…¼ë¬¸](https://arxiv.org/abs/2203.02155)
- [Constitutional AI](https://arxiv.org/abs/2212.08073)

---

ê°•í™”í•™ìŠµìœ¼ë¡œ ë” ë˜‘ë˜‘í•œ LLM ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”! ğŸ¤–ğŸš€

ì§ˆë¬¸ì´ë‚˜ ì œì•ˆì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.
